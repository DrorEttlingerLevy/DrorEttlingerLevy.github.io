---
title: 'Training and Evaluating on the Training Set'
date: 2025-02-16
permalink: /posts/2025/02/Training/
image: /images/blog/03072025_forest.png
preview: >
  Why Cross-Validation & Grid Search Matter 🔍 `Cross-validation` prevents overfitting by splitting training data into multiple folds, ensuring the model generalizes well. Without it, models like **Decision Trees** may seem perfect but fail on unseen data. 📉 `Grid Search` fine-tunes models by testing multiple hyperparameter combinations, finding the best-performing setup. Instead of guessing, it systematically optimizes models like **Random Forests** for better accuracy. ✨
header:
  teaser: /blog/03072025_forest.png
---
Why Cross-Validation & Grid Search Matter 🔍 `Cross-validation` prevents overfitting by splitting training data into multiple folds, ensuring the model generalizes well. Without it, models like **Decision Trees** may seem perfect but fail on unseen data. 📉 `Grid Search` fine-tunes models by testing multiple hyperparameter combinations, finding the best-performing setup. Instead of guessing, it systematically optimizes models like **Random Forests** for better accuracy. ✨


# The Fun Part - Selecting and Training the Model 
* Most of this learning post is from the awesome book - 'Hands On Machine Learning' by Aurelien Geron.

After understating and data, the business question and cleaning with pipelines, lets focus about ML models 🏄‍♀️

## Training and Evaluating on the Training Set
---

For simplicity start with Linear Regression
```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(transform_train_set, y_train)

some_data = transform_train_set[:5]
some_data_labels = y_train.iloc[:5]
results_df = pd.DataFrame({'pred': lin_reg.predict(some_data), 'acutal':some_data_labels})
results_df
```
And measure this model by using RMSE

```python
from sklearn.metrics import mean_squared_error
train_pred = lin_reg.predict(transform_train_set)
lin_mse = mean_squared_error(y_train, train_pred)
lin_rmse = np.sqrt(lin_mse)
lin_rmse
```

For this example I used housing data, and the result for linear regression is 68,866$ , which is a lot considering the house price (around 120,000 -265,000)

If I try a more robust model like regressor decision tree:

```python
from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor()
train_pred_tree = tree_reg.fit(transform_train_set, y_train)

train_pred_tree = tree_reg.predict(transform_train_set)
tree_mse = mean_squared_error(train_pred_tree, y_train)
tree_rmse = np.sqrt(tree_mse)
tree_rmse
```

**The rmse is 0.0**,  Can it be that the result is so perfect? likely not... the model was given the **whole** training data, so it over fit it. 
## Cross-Validation
---
Let try K-fold cross-validation to randomly split the training set to *folds*, each time picking a fold and evaluating it on the other 9 folds.
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(estimator=tree_reg, X=transform_train_set, y=y_train,
                         scoring='neg_mean_squared_error', cv=10)
tree_rmse_scores = np.sqrt(-scores)
tree_rmse_scores
```
>💡 **How did it helped?**
> 
> By using Cross-Validation the model can `overfit` only on 1/10 of ths train data, and so it have to generalize for the other folds.
> Also we can see the std of the Cross-Validation scores thus understand how precise the estimation is.
<div class="note">
    <strong>Note:</strong> Scikit-Learn's Cross-Validation is expecting utility function (greater is better) and not cost function (lower is better)
    so we need to use (-scores) for evaluation RMSE.
</div>

The mean for the RMSE of the Regressor Decision Tree is: 69,343$ --> which is worse compered to the `Linear Regression`, this is due to overfitting.

Let's assume we have a list of a few models we want to try, how can we fine-tune them?

## Grid Search
---
Grid search is iterating over different hyperparameter and tries all of them it on each fold.
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

param_grid = [
  {'n_estimators':[3, 10, 30],'max_features':[2, 4, 6, 8]},
  {'bootstrap':[False], 'n_estimators':[3, 10], 'max_features': [2, 3, 4]},
]

forest_reg = RandomForestRegressor()

grid_search = GridSearchCV(estimator= forest_reg, param_grid= param_grid,
                           cv=5, scoring='neg_mean_squared_error', return_train_score=True)

grid_search.fit(transform_train_set, y_train)
```
We can see all the results for all combinations:
```python
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):
  print(np.sqrt(-mean_score), params)
```

Next, I plan to explore more models, use **RandomizedSearchCV** for faster tuning, and build a **preparation pipeline** to select key attributes. I'll also streamline everything into **one unified pipeline** for efficiency! 🎯



