---
title: 'Batch Normalization in Deep Learning - Stabilization, Scaling, and Undoing'
date: 2025-06-30
permalink: /posts/2025/06/Batch_normalization/
image: /images/blog/30062025_1.png
preview: >
  Batch Normalization speeds up training and stabilizes deep networks by normalizing layer outputs, not just inputs. It can also be canceled by some mathematical tricks.
header:
  teaser: /blog/30062025_1.png
---

**Batch Normalization** helps neural networks train *faster* and *more stable* by normalizing the outputs of each layer â€” not just the inputs.

<div class="note">
  <strong>Note:</strong> Batch normalization is applied during training and adjusts outputs using learned parameters to maintain expressive power.
</div>

## â“ Why Normalize?

â€˜Normalizationâ€™ means squeezing values into a range like `[0, 1]`, and â€˜standardizationâ€™ makes the data have mean `0` and standard deviation `1`.

When features are in different scales, the model **struggles to learn** â€” gradients can explode or vanish.

<div class="info">
  <strong>Important:</strong> Batch norm doesnâ€™t just apply to inputs. Itâ€™s applied across **each layerâ€™s activations**, and it's done for every mini-batch.
</div>

So instead of normalizing once, we normalize *throughout the network*, layer-by-layer.

![Layer Normalization Example](/images/blog/30062025.png)

**Source:**  
[AssemblyAI â€“ What it is and how to implement it](https://www.youtube.com/watch?v=yXOMHOpbon8&ab_channel=AssemblyAI)  
[CodeEmporium â€“ EXPLAINED!](https://www.youtube.com/watch?v=DtEq44FTPM4&ab_channel=CodeEmporium)

---

## ğŸ” Why Scale and Shift After Normalization?

Just normalizing forces the outputs to have `mean = 0` and `std = 1`, which can limit the networkâ€™s flexibility.

So instead, we **add two trainable parameters**, `Î³áµ¢` (scale) and `Î²áµ¢` (shift), so the network can learn how much to adjust the normalization.

<div class="note">
  <strong>Note:</strong> These two values allow the model to undo the normalization if needed â€” or learn a new better one for the task.
</div>

---

## ğŸ”„ Undoing Batch Normalization?

Letâ€™s say we want to go back to the original `xáµ¢`, like batch norm didnâ€™t happen.

The full expression is:

$$
\hat{h}_i = \gamma_i \cdot \frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}} + \beta_i
$$

To cancel batch norm, we want:

$$
\hat{h}_i = x_i
$$

So:

$$
x_i = \gamma_i \cdot \frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}} + \beta_i
$$

Multiply both sides:

$$
x_i \cdot \sqrt{\sigma^2 + \varepsilon} = \gamma_i(x_i - \mu) + \beta_i \cdot \sqrt{\sigma^2 + \varepsilon}
$$

Now match the coefficients:

$$
\gamma_i = \sqrt{\sigma^2 + \varepsilon}, \quad \beta_i = \mu
$$

Substitute:

$$
\hat{h}_i = x_i
$$

<div class="info">
  <strong>Key Insight:</strong> You can fully cancel Batch Norm by setting <code>Î³áµ¢ = âˆš(ÏƒÂ² + Îµ)</code> and <code>Î²áµ¢ = Î¼</code>.
</div>

---

## ğŸ§® Batch Norm Computational Graph

Hereâ€™s how the operations flow inside a BN layer:

![Computational Graph](/images/blog/30062025_1.png)

First, we calculate `Î¼` and `ÏƒÂ²` for the current batch. Then normalize `xáµ¢`, and apply the learned `Î³` and `Î²`.

During inference, we donâ€™t use the batch stats anymore â€” instead, we use **running averages** collected during training.

---

**In summary**, Batch Normalization makes training more efficient, smoother, and often gives better generalization.  
But whatâ€™s nice is that itâ€™s still flexible â€” the network can learn to apply it fully, partially, or *not at all* depending on the task.